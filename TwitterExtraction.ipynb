{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation analysis between the Bitcoin currency and Twitter\n",
    "\n",
    "This project consists of a correlation analysis between the Bitcoin currency and tweets. In order to define the positiveness of a tweet (if the course of the bitcoin will go up or down), we realise a sentiment analysis of each tweet using the VADER algorithm. Finally we try to find a correlation between the two and we will make some machine learning to make predictions.\n",
    "\n",
    "This notebook was written using Python 3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Twython\n",
    "We use the *twython* package as my Python interface with the Twitter API: https://twython.readthedocs.io/en/latest/usage/starting_out.html\n",
    "\n",
    "The twython package must be installed using *pip install twython* from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OAuth2 Authentication (*app* authentication)\n",
    "Here we use the method *OAuth2* along with the Twithon library to authenticate on the twitter API.\n",
    "\n",
    "OAuth1 will give you *user* access to the API, whereas OAuth2 will give the *app* access. For academic use the rate limits are generally better for *OAuth2* (app) authentication, with a few exceptions. For a chart showing the API limits for user and app authentication for the various parts of the Twitter API, see this chart: https://dev.twitter.com/rest/public/rate-limits\n",
    "\n",
    "Running the code block below shows that we now have a rate limit of 450 API calls. This means we can make 450 different calls to the API within the current 15-minute window. With the search API we can access 100 tweets per call. This means that, if we were downloading tweets with a specific hashtag, such as *#arnova16*, we could download 450 $\\times$ 100 or 45,000 tweets per window. This is much better than the 18,000 tweets we can access using the OAuth1 or user authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_KEY = 'mPQKoRwd2Pb9qpQyQmyG5s8KR'\n",
    "APP_SECRET = 'HLvIhusvfzDLKaRXY8CnZGP143kp3E3f2KqQBIEMfVL5mOxZjq'\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "twitter.get_application_rate_limit_status()['resources']['search']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the twitter API\n",
    "Here we query the twitter API to get the latest tweets about bitcoin. Then we transform it to store only the useful data inside a Pandas Dataframe.\n",
    "\n",
    "The pandas package must be installed using *pip install pandas* from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 33052, waiting for 15 minutes until next queries\n",
      "Retrieved 32952, waiting for 15 minutes until next queries\n",
      "Retrieved 33279, waiting for 15 minutes until next queries\n",
      "Retrieved 32717, waiting for 15 minutes until next queries\n",
      "Retrieved 33609, waiting for 15 minutes until next queries\n",
      "Retrieved 33143, waiting for 15 minutes until next queries\n",
      "Retrieved 33286, waiting for 15 minutes until next queries\n",
      "Retrieved 33613, waiting for 15 minutes until next queries\n",
      "Retrieved 33513, waiting for 15 minutes until next queries\n",
      "Retrieved 33190, waiting for 15 minutes until next queries\n",
      "Retrieved 33182, waiting for 15 minutes until next queries\n",
      "Retrieved 34027, waiting for 15 minutes until next queries\n",
      "Retrieved 29456, waiting for 15 minutes until next queries\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a9e8053b8695>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"statuses\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'#bitcoin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mixed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"100\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"statuses\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mnext_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"statuses\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"statuses\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Retrieved {0}, waiting for 15 minutes until next queries'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"statuses\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_QUERIES = 400\n",
    "data = {\"statuses\": []}\n",
    "next_id = \"\"\n",
    "f = open('tweets_raw.csv', 'a', encoding='utf-8')\n",
    "while(True):\n",
    "    for i in range(NUMBER_OF_QUERIES):\n",
    "        if not next_id:\n",
    "            data = twitter.search(q='#bitcoin', lang='en', result_type='recent', count=\"100\") # Use since_id for tweets after id\n",
    "        else:\n",
    "            data[\"statuses\"].extend(twitter.search(q='#bitcoin', lang='en', result_type='mixed', count=\"100\", max_id=next_id)[\"statuses\"])\n",
    "        next_id = data[\"statuses\"][len(data[\"statuses\"]) - 1]['id']\n",
    "        \n",
    "    print('Retrieved {0}, waiting for 15 minutes until next queries'.format(len(data[\"statuses\"])))\n",
    "    #print(data[\"statuses\"][0][\"favorite_count\"])\n",
    "    if len(data[\"statuses\"]) == 0:\n",
    "        break\n",
    "    else:\n",
    "        d = pd.DataFrame([[s[\"id\"], s[\"text\"].replace('\\n','').replace('\\r',''), s[\"user\"][\"name\"], s[\"user\"][\"followers_count\"], s[\"retweet_count\"], s[\"favorite_count\"], s[\"created_at\"]] for s in data[\"statuses\"]], columns=('ID', 'Text', 'UserName', \"UserFollowerCount\", 'RetweetCount', 'Likes', \"CreatedAt\"))\n",
    "        d.to_csv(f, mode='a', header=True, encoding='utf-8',index=False)\n",
    "        data[\"statuses\"] = []\n",
    "    \n",
    "    sleep(910)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now we will cleanup the data.\n",
    "\n",
    "We already filtered tweets in english in the call to the Twitter API.\n",
    "We will now filter links, @Pseudo, images, videos, unhashtag #happy -> happy.\n",
    "\n",
    "We won't transform to lower case because Vader take capital letters into consideration to emphasize sentiments.\n",
    "\n",
    "You must install `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 429007/429007 [11:04:06<00:00, 10.77it/s]  \n"
     ]
    }
   ],
   "source": [
    "import re # regular expressions\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "d = pd.read_csv('tweets_raw.csv')\n",
    "for i,s in enumerate(tqdm(d['Text'])):\n",
    "    text = d.loc[i, 'Text']\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = re.sub('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('@\\\\w+ *', '', text, flags=re.MULTILINE)\n",
    "    d.loc[i, 'Text'] = text\n",
    "f = open('tweets_clean.csv', 'a', encoding='utf-8')\n",
    "d.to_csv(f, header=True, encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('tweets_clean.csv')\n",
    "df_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                  992434490168496133\n",
       "Text                  bitcoin sees wall street warm to trading virt...\n",
       "UserFollowerCount                                                    0\n",
       "RetweetCount                                                         0\n",
       "CreatedAt                               Fri May 04 16:03:15 +0000 2018\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
